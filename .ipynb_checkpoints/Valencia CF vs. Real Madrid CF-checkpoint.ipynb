{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RZB/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: latin-1 -*-\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import geocoder\n",
    "\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "sa = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'rmdvv.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-368cf0f0b1c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Open and read in the tweets file as JSON objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rmdvv.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtweets_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtweets_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'rmdvv.txt'"
     ]
    }
   ],
   "source": [
    "## Open and read in the tweets file as JSON objects\n",
    "\n",
    "with open('rmdvv.txt', 'r') as tweets_text:\n",
    "    tweets_data = []\n",
    "    for line in tweets_text:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            if tweet['text'] is not None:\n",
    "                tweets_data.append(tweet)\n",
    "            else:\n",
    "                print 'Caught'\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## How many tweets do we have in our collection?\n",
    "\n",
    "\n",
    "print len(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create our dataframe and parse each tweet for the text, language, date/time created, and coordinates if available.\n",
    "\n",
    "\n",
    "tweets_df = pd.DataFrame()\n",
    "tweets_df['text'] = map(lambda tweet: tweet.get('text'), tweets_data)\n",
    "tweets_df['lang'] = map(lambda tweet: tweet.get('lang',None).encode('utf-8'), tweets_data)\n",
    "tweets_df['created_at'] = map(lambda tweet: tweet.get('created_at',None), tweets_data)\n",
    "tweets_df['coordinates'] = map(lambda tweet: tweet.get('coordinates','None'), tweets_data)\n",
    "\n",
    "## Define a function that parses the time per tweet and determines what time period the tweet was sent '\n",
    "##  'Half time' 'Second Half' 'Full time'\n",
    "\n",
    "def time_parser(df):\n",
    "    new_time = df['created_at'].split('Jan 03 ')[1].split(' +')[0].replace(':', '')\n",
    "    if int(new_time) <= 201900:\n",
    "        time_class = 'First half'\n",
    "    elif 201900 < int(new_time) <= 203100:\n",
    "        time_class = 'Half time'\n",
    "    else:\n",
    "        time_class = 'Second half'\n",
    "\n",
    "    return time_class \n",
    "\n",
    "## Define a function to clean the tweets, preparing them for the sentiment analysis\n",
    "\n",
    "def tweet_cleaner(df):\n",
    "    text = df['text'].lower().encode('unicode_escape')\n",
    "    new_tweet = \" \".join([word for word in text.split()\n",
    "                            if 'http' not in word and '\\u' not in word and '\\U' not in word and \"\\n\" not in word \n",
    "                                and not word.startswith('\"@')\n",
    "                                and not word.startswith('@')\n",
    "                                and not word.startswith('#')\n",
    "                                and not word.startswith('\\U')\n",
    "                                and not word.startswith('\\n')\n",
    "                                and word != 'rt'\n",
    "                                and word != '&amp;'\n",
    "                            ])\n",
    "    return new_tweet\n",
    "\n",
    "## Apply functions to dataframe\n",
    "\n",
    "tweets_df['parsed_time'] = tweets_df.apply(time_parser, axis = 1)\n",
    "tweets_df['cleaned_tweet'] = tweets_df.apply(tweet_cleaner, axis = 1)\n",
    "\n",
    "\n",
    "##Parse for user reported locations\n",
    "\n",
    "locations = []\n",
    "for tweet in tweets_data:\n",
    "    try:\n",
    "        if tweet['user']:\n",
    "            locations.append(tweet['user']['location'])\n",
    "        else:\n",
    "            locations.append('')\n",
    "    except KeyError:\n",
    "        locations.append('')\n",
    "        continue\n",
    "\n",
    "##Parse for geocoded countries        \n",
    "        \n",
    "countries = []\n",
    "for tweet in tweets_data:\n",
    "    try:\n",
    "        if tweet['place']:\n",
    "            try:\n",
    "                countries.append(tweet['place']['country'].encode('unicode_escape'))\n",
    "            except AttributeError:\n",
    "                continue\n",
    "        else:\n",
    "            countries.append('')\n",
    "    except KeyError:\n",
    "        countries.append('')\n",
    "        continue\n",
    "\n",
    "##Parse for followers count        \n",
    "\n",
    "followers = []\n",
    "for tweet in tweets_data:\n",
    "    try:\n",
    "        if tweet['user']:\n",
    "            followers.append(tweet['user']['followers_count'])\n",
    "        else:\n",
    "            locations.append('')\n",
    "    except KeyError:\n",
    "        followers.append('')\n",
    "        continue\n",
    "\n",
    "##Parse for username        \n",
    "        \n",
    "username = []\n",
    "for tweet in tweets_data:\n",
    "    try:\n",
    "        if tweet['user']:\n",
    "            username.append(tweet['user']['screen_name'])\n",
    "        else:\n",
    "            username.append('')\n",
    "    except KeyError:\n",
    "        username.append('')\n",
    "        continue\n",
    "\n",
    "##Parse for retweet check        \n",
    "        \n",
    "retweets = []\n",
    "\n",
    "tweets = tweets_df.iterrows()\n",
    "\n",
    "for tweet in tweets:\n",
    "    if tweet[1][0].startswith('RT'):\n",
    "        retweets.append('Y')\n",
    "    else:\n",
    "        retweets.append('N') \n",
    "\n",
    "\n",
    "## Add columns\n",
    "        \n",
    "tweets_df['username'] = username\n",
    "tweets_df['country'] = countries\n",
    "tweets_df['locations'] = locations\n",
    "tweets_df['followers_count'] = followers\n",
    "tweets_df['count'] = 1\n",
    "tweets_df['retweet'] = retweets\n",
    "\n",
    "## Validate counts\n",
    "\n",
    "print len(followers)  \n",
    "print len(username)\n",
    "print len(countries)\n",
    "print len(tweets_df)\n",
    "print len(tweets_df['coordinates'].dropna())\n",
    "tweets_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create coordinates dataframe\n",
    "\n",
    "cord_df = tweets_df[tweets_df.coordinates != 'None'].dropna()\n",
    "\n",
    "## Define latitude and longitude parsing functions\n",
    "\n",
    "def lat_parse(df):\n",
    "    lat = str(df['coordinates'])\n",
    "    lat = lat.split(\"'coordinates': [\")[1].split(']')[0]\n",
    "    lat = lat.split(',')[0]\n",
    "    return lat\n",
    "\n",
    "def lon_parse(df):\n",
    "    lon = str(df['coordinates'])\n",
    "    lon = lon.split(\"'coordinates': [\")[1].split(']')[0]\n",
    "    lon = lon.split(',')[1]\n",
    "    return lon\n",
    "\n",
    "# Apply functions\n",
    "\n",
    "cord_df['lat'] = cord_df.apply(lat_parse, axis = 1)\n",
    "cord_df['lon'] = cord_df.apply(lon_parse, axis = 1)\n",
    "cord_df[['lat', 'lon']]\n",
    "\n",
    "\n",
    "cord_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "team_class = []\n",
    "rmcf = []\n",
    "vacf = []\n",
    "no_pref = []\n",
    "na = []\n",
    "\n",
    "## Parse tweets for team context\n",
    "## 'No pref' = Both teams/ No teams\n",
    "## 'VACF' = Valencia\n",
    "## 'RMCF' = Real Madrid\n",
    "## 'N/A' = Not english\n",
    "\n",
    "iterd = tweets_df.iterrows()\n",
    "for t in iterd:\n",
    "    try:\n",
    "        if 'valencia' in t[1][0].lower() and 'madrid' not in t[1][0].lower() and t[1][1] == 'en':\n",
    "            team_class.append('VACF')\n",
    "            vacf.append('VACF')\n",
    "        elif 'madrid' in t[1][0].lower() and 'valencia' not in t[1][0].lower() and t[1][1] == 'en':\n",
    "            team_class.append('RMCF')\n",
    "            rmcf.append('RMCF')\n",
    "        elif t[1][1] != 'en':\n",
    "            team_class.append('N/A')\n",
    "            na.append('N/A')\n",
    "        else:\n",
    "            team_class.append('No pref')\n",
    "            no_pref.append('no pref')\n",
    "    except AttributeError:\n",
    "        print 'Caught'\n",
    "        \n",
    "tweets_df['team_class'] = team_class\n",
    "\n",
    "iterrer = tweets_df.iterrows()\n",
    "rt = []\n",
    "rt_count = []\n",
    "for t in iterrer:\n",
    "    if t[1][0].startswith('RT'):\n",
    "        rt.append('Y')\n",
    "        rt_count.append('rt')\n",
    "    else:\n",
    "        rt.append('N')\n",
    "print len(rt_count)\n",
    "\n",
    "tweets_df['RT_binary'] = rt\n",
    "        \n",
    "print len(team_class)\n",
    "print len(rmcf)\n",
    "print len(vacf)\n",
    "print len(no_pref)\n",
    "print len(na)\n",
    "print len(rt_count)\n",
    "tweets_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Language distribution\n",
    "\n",
    "lang_count = tweets_df['lang'].value_counts()\n",
    "l_df = pd.DataFrame(lang_count, columns = ['count'])[0:10]\n",
    "l_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot language distribution\n",
    "\n",
    "label_size = 25\n",
    "mpl.rcParams['xtick.labelsize'] = label_size\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.title('Language Distribution', size = 20)\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "langplt = sns.countplot(x = 'lang', palette= 'Greens_r', data = tweets_df, order=['es','en','pt','in','und', 'fr', 'tr'])\n",
    "plt.xlabel('Languages')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "for p in langplt.patches:\n",
    "        langplt.annotate(int(p.get_height()), (p.get_x() + 0.2, p.get_height() + 50), size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnty_count = tweets_df['country'].value_counts()\n",
    "c_df = pd.DataFrame(cnty_count, columns = ['count'])[1:10]\n",
    "top10 = c_df.index\n",
    "print top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot geolocated country distribution\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.title('Geolocated Country Distribution', size = 20)\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "cntplot = sns.countplot(x = 'country', palette= 'Reds_r', data = tweets_df,order= top10)\n",
    "for item in cntplot.get_xticklabels():\n",
    "    item.set_rotation(45)\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "for p in cntplot.patches:\n",
    "        cntplot.annotate(int(p.get_height()), (p.get_x()+0.2, p.get_height()+ 0.5), size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_loc = tweets_df['locations'].value_counts()\n",
    "loc_df = pd.DataFrame(user_loc, columns = ['count'])[0:10]\n",
    "top10 = loc_df.index[0:10]\n",
    "print top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot user reported locations distribution\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.title('Top 10 User Reported Locations', size = 20)\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "usrlocplot = sns.countplot(x = 'locations', palette= 'Blues_r',data = tweets_df, order= top10)\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Count')\n",
    "for item in usrlocplot.get_xticklabels():\n",
    "    item.set_rotation(45)\n",
    "\n",
    "for p in usrlocplot.patches:\n",
    "        usrlocplot.annotate(int(p.get_height()), (p.get_x()+0.2, p.get_height()+ 0.5), size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot team classification distribution\n",
    "\n",
    "plt.figure(figsize = (9,9))\n",
    "plt.title('Team Class', size = 20)\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "tclassplot = sns.countplot(x = 'team_class',palette= 'Purples_r',data = tweets_df)\n",
    "plt.xlabel('Team')\n",
    "plt.ylabel('Count')\n",
    "for item in tclassplot.get_xticklabels():\n",
    "    item.set_rotation(45)\n",
    "\n",
    "for p in tclassplot.patches:\n",
    "        tclassplot.annotate(int(p.get_height()), (p.get_x()+0.2, p.get_height()+ 50), size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cord_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot geolocated tweets\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "m = Basemap(projection='mill',llcrnrlat= -70,urcrnrlat=86,\\\n",
    "                llcrnrlon=-170,urcrnrlon=170,resolution='c')\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "m.drawmapboundary(fill_color='#FFFFFF')\n",
    "m.bluemarble()\n",
    "\n",
    "corditer = cord_df.iterrows()\n",
    "\n",
    "re_geo_df = pd.DataFrame()\n",
    "geo_cit = []\n",
    "geo_cnty = []\n",
    "\n",
    "for c in corditer:\n",
    "    lat = float(c[1][12])\n",
    "    lon = float(c[1][13])\n",
    "    \n",
    "    g = geocoder.google([lon, lat], method='reverse')\n",
    "    #print g.city,', ',g.country_long\n",
    "    \n",
    "    geo_cit.append(g.city)\n",
    "    geo_cnty.append(g.country_long)\n",
    "    \n",
    "    xpt,ypt = m(lat,lon)\n",
    "    m.plot(xpt,ypt, 'ro', markersize= 7)\n",
    "\n",
    "\n",
    "plt.title(\"Geolocations Plot\", size = 25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Dataframe of re-geolocated lat and lons to actual location names\n",
    "\n",
    "re_geo_df['City'] = geo_cit\n",
    "re_geo_df['Country'] = geo_cnty\n",
    "\n",
    "re_geo_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Define a function that parses the time per tweet and determines which 5 minute interval\n",
    "##the tweet was sent during.\n",
    "\n",
    "def minute_parser(df):\n",
    "    new_time = df['created_at'].split('Jan 03 ')[1].split(' +')[0].replace(':', '')\n",
    "    if 193000 < int(new_time) <= 193500:\n",
    "        time_class = 5\n",
    "        return time_class\n",
    "    elif 193500 < int(new_time) <= 194000:\n",
    "        time_class = 10\n",
    "        return time_class\n",
    "    elif 194000 < int(new_time) <= 194500:\n",
    "        time_class = 15\n",
    "        return time_class\n",
    "    elif 194500 < int(new_time) <= 195000:\n",
    "        time_class = 20\n",
    "        return time_class\n",
    "    elif 195000 < int(new_time) <= 195500:\n",
    "        time_class = 25\n",
    "        return time_class\n",
    "    elif 195500 < int(new_time) <= 200000:\n",
    "        time_class = 30\n",
    "        return time_class\n",
    "    elif 200000 < int(new_time) <= 200500:\n",
    "        time_class = 35\n",
    "        return time_class\n",
    "    elif 200500 < int(new_time) <= 201000:\n",
    "        time_class = 40\n",
    "        return time_class\n",
    "    elif 201000 < int(new_time) <= 201700:\n",
    "        time_class = 45\n",
    "        return time_class\n",
    "    elif 201700 < int(new_time) <= 202000:\n",
    "        time_class = 'Half time'\n",
    "        return time_class\n",
    "    elif 202000 < int(new_time) <= 202500:\n",
    "        time_class = 'Half time'\n",
    "        return time_class\n",
    "    elif 202500 < int(new_time) <= 203000:\n",
    "        time_class = 'Half time'\n",
    "        return time_class\n",
    "    elif 203000 < int(new_time) <= 203500:\n",
    "        time_class = 50\n",
    "        return time_class\n",
    "    elif 203500 < int(new_time) <= 204000:\n",
    "        time_class = 55\n",
    "        return time_class\n",
    "    elif 204000 < int(new_time) <= 204500:\n",
    "        time_class = 60\n",
    "        return time_class\n",
    "    elif 204500 < int(new_time) <= 205000:\n",
    "        time_class = 65\n",
    "        return time_class\n",
    "    elif 205000 < int(new_time) <= 205600:\n",
    "        time_class = 70\n",
    "        return time_class\n",
    "    elif 205600 < int(new_time) <= 210000:\n",
    "        time_class = 75\n",
    "        return time_class\n",
    "    elif 210000 < int(new_time) <= 210500:\n",
    "        time_class = 80\n",
    "        return time_class\n",
    "    elif 210500 < int(new_time) <= 211000:\n",
    "        time_class = 85\n",
    "        return time_class\n",
    "    elif 211000 < int(new_time) <= 211830:\n",
    "        time_class = 90\n",
    "        return time_class\n",
    "    else:\n",
    "        return 'none'\n",
    "        print 'NA'\n",
    "\n",
    "\n",
    "tweets_df['parsed_minute'] = tweets_df.apply(minute_parser, axis = 1)\n",
    "tweets_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot tweet frequencies per 5 minute intervals\n",
    "\n",
    "plt.figure(figsize = (9,9))\n",
    "plt.title('Tweet Frequency', size = 20)\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "tclassplot = sns.countplot(x = 'parsed_minute', palette= 'Purples_r',data = tweets_df)\n",
    "plt.xlabel('Minute')\n",
    "plt.ylabel('Tweet Count')\n",
    "for item in tclassplot.get_xticklabels():\n",
    "    item.set_rotation(50)\n",
    "\n",
    "for p in tclassplot.patches:\n",
    "        tclassplot.annotate(int(p.get_height()), (p.get_x(), p.get_height()+ 50), size = 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Heatmap of tweet frequencies per team per 5 minute interval\n",
    "\n",
    "df = tweets_df[tweets_df.parsed_minute != 'Half time']\n",
    "dff = df[df.team_class != 'N/A']\n",
    "dfff = dff[dff.team_class != 'No pref']\n",
    "\n",
    "nahm_pivot = dfff.pivot_table('count',\n",
    "                                index='team_class',\n",
    "                                columns='parsed_minute', aggfunc='sum')\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.title(\"Tweet Frequency by 5' Intervals\", size = 20)\n",
    "sns.heatmap(nahm_pivot, annot= True, fmt= '.9g', linewidths=.5)\n",
    "plt.xlabel('Minute')\n",
    "plt.ylabel('Team')\n",
    "plt.figtext(.9, 0.8, \"Time Distribution:\")\n",
    "plt.figtext(.95, 0.77, \"Half time: \")\n",
    "plt.figtext(.95, 0.74, \"Second half:\" )\n",
    "plt.figtext(.95, 0.71, \"Full time:\" )\n",
    "plt.figtext(.9, 0.6, \"Sentiment Averages:\")\n",
    "plt.figtext(.95, 0.57, \"Half time:\" )\n",
    "plt.figtext(.95, 0.54, \"Second half:\" )\n",
    "plt.figtext(.95, 0.51, \"Full time: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define sentiment analysis function\n",
    "\n",
    "def sentiment_analyzer(df):\n",
    "    if df['lang'] == 'en':\n",
    "        tweet = str(df['cleaned_tweet'])\n",
    "        score = float(round(sa(tweet).sentiment[1], 2))\n",
    "        return score\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tweets_df['sent_score'] = tweets_df.apply(sentiment_analyzer, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Plot out sentiment distribution of tweets from our collection\n",
    "\n",
    "## Sentiment is plotted on a scale of 0 (most negative) to (1 most positive)\n",
    "va_df = tweets_df[tweets_df.team_class == 'VACF']\n",
    "rm_df = tweets_df[tweets_df.team_class == 'RMCF']\n",
    "np_df = tweets_df[tweets_df.team_class == 'No pref']\n",
    "plt.figure(figsize = (9,9))\n",
    "plt.title('Sentiment Distribution', size = 20)\n",
    "\n",
    "sent_distplot = sns.kdeplot(np_df.sent_score.dropna(), shade = True)\n",
    "\n",
    "sns.kdeplot(va_df.sent_score.dropna(), shade = True)\n",
    "sns.kdeplot(rm_df.sent_score.dropna(), shade = True)\n",
    "plt.xlim([0, 1])\n",
    "plt.legend(['No pref', 'VACF', 'RMCF'], loc= 'best')\n",
    "plt.ylim([0,3.5])\n",
    "\n",
    "plt.xlabel('Sentiment score')\n",
    "\n",
    "\n",
    "\n",
    "print 'General sentiment mean: ' +  str(tweets_df.sent_score.dropna().mean())\n",
    "print 'Real Madrid sentiment mean: ' + str(rm_df.sent_score.dropna().mean())\n",
    "print 'Valencia sentiment mean: ' +  str(va_df.sent_score.dropna().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
